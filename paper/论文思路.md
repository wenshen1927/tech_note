## 基于GPU-DMM模型的微博主题演化分析
1. 针对微博的短文本特性，提出使用GPU-DMM模型来做主题抽取；
2. 根据聚类（某个聚类方法）来确定两个主题是否有关联

## 问题
1. 如何确定主题强度？
2. 如何分析主题内容的演化：主题的产生、主题的消亡、主题的强度。主题内容向其他主题的迁移或渗透。主题的属性的变化。（主要一是主题在时间轴上的强度变化,即主题的活跃度变化;二是主题在内容上的变化, 即主题的内容迁移。）
3. LDA模型的描述。
4. 针对短文本的特性，更换LDA模型为**GPU-DMM**。
5. 还需要再看20篇左右的论文精读。


## 思路：
### 思路一：
1. 将微博新闻按时间的粒度（天、几个小时）分为时间片。对每个时间片的新闻进行LDA（改进的模型）抽取主题。
2. 针对微博的特点：
    1. 微博内容变化快、粒度细——>提出在线的LDA方法
    2. 微博的短文本特性,极强的上下文相关性——>提出短文本LDA（GPU-DMM等）
    3. 主题模型解决短文本的方式：扩充短文本特征，背景语料训练
3. 主题强度演化分析：主题强度定义为一个时间片内，主题簇出现的词数
4. 主题相似度：根据主题词的概率进行相似度计算，并根据相似度阈值进行主题是否关联的划分。
5. 根据相似度把主题进行聚类，聚类为多个簇，认为同一个聚类簇是一个主题。聚类方法根据相似度计算，并设置阈值。
6. 主题内容演化：根据主题强度来分析主题的出现，存在，消亡。（还需要补充）

### 思路二:
LDA相当于给文本降低纬度，降低维度之后提取出的主题词一定程度上能够代表这一篇文档，将主题词转换为词向量，并且把主题词的词向量拼接起来，达到句子级别，再计算句子级别的相似度，相当于计算文本的相似度。其中的理论支撑需要再研究。为什么词向量能够更好地计算文档相似度。这样的话要求各文档提取出的主题词个数要相等，这个假设也值得商榷，能不能提出主题数无关的词向量计算方法（记得有一篇paper写了方法）。

### 思路三：
 + 结合词向量技术，使用加入时间因素的ETM模型（类似TOT模型）进行建模(学习一下TOT怎么把时间因素加入到LDA中，类似地把时间因素加入到ETM中，所以目前首先需要学会TOT的推倒和ETM的原理及推倒，最后把两个方法融合)
 + 如何设计实验

## 主要创新点
1. 针对微博特点提出在线的针对短文本的LDA模型。

## 接下来要做的
1. 理解LDA，理解OLDA，理解GPU-DMM，并进行试验。（一周）,理解相关的短文本主题模型
2. 爬取微博语料（一个月内的），进行时间划分，便与实验。（2天）并做好语料内容理解。
3. 进行一个完整的主题演化实验，使用最简单的LDA模型，熟悉流程。（3天）
4. 理解困惑度的概念和公式。


+ 理解ETM（词向量LDA）和TOT模型（加入时间因素的LDA）
+ 想办法在ETM中加入时间因素，做好公式推倒
+ 在上面做好的基础上想好如何设计实验

## 实验顺序：
1. 普通LDA的主题演化实验
2. 短文本LDA主题演化实验
3. 短文本改建LDA主题演化实验
